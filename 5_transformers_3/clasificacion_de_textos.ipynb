{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6545b65-5394-4375-826e-94953c55e76b",
   "metadata": {},
   "source": [
    "# Generaci√≥n de textos con ü§ó Transformers\n",
    "\n",
    "> C√≥mo generar texto con diferentes m√©todos de descodificaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13c35a-5d60-4c4b-8f01-e249fbbf8d32",
   "metadata": {},
   "source": [
    "En este cuaderno exploraremos varios m√©todos para generar texto con una versi√≥n espa√±ola de GPT-2. Estos modelos se denominan \"modelos decodificadores\" y, si no est√°s familiarizado con ellos, te recomendamos que veas primero el v√≠deo que aparece a continuaci√≥n:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749879c3-fd1d-4f52-ac2b-d967a6c64c60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cargar y explorar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4c917-165d-43b9-924e-47782adbd6c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuraci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05ee00-b4ec-4ae5-8b49-bf2531f3d666",
   "metadata": {},
   "source": [
    "Si est√° ejecutando este notebook en Google Colab, ejecute la siguiente celda para instalar las bibliotecas que necesitamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc194002-5d32-4b46-a8c8-471d3a8af1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "799416ea-3eaa-4f99-a966-7983e83e4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "    \n",
    "def display_df(df, max_cols=15, header=True, index=True):\n",
    "    return display(HTML(df.to_html(header=header,index=index, max_cols=max_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6780287-bb24-426a-b467-25191e99759f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Greedy search decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7a3c-36b7-4066-a976-4d12c58ba4c3",
   "metadata": {},
   "source": [
    "El m√©todo de decodificaci√≥n m√°s sencillo para obtener fichas discretas a partir de la salida continua de un modelo es seleccionar con \"avidez\" la ficha con mayor probabilidad en cada paso de tiempo:\n",
    "\n",
    "$$ \\hat{y}_t =  \\underset{y_t}{\\operatorname{argmax}} P(y_t | y_{<t}, \\mathbf{x}) \\,.$$\n",
    "\n",
    "Veamos c√≥mo funciona bajo el cap√≥ con una versi√≥n espa√±ola de GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d183a933-15c4-4a8f-a922-6ffe36db0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"mrm8488/spanish-gpt2\" #\"flax-community/gpt-2-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# A√±adimos el token EOS como token PAD para evitar warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e16807f-968a-43d4-92f6-6beb7d869a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del GPT espa√±ol: 124.4M par√°metros\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"Tama√±o del GPT espa√±ol: {model_size(model)/1000**2:.1f}M par√°metros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcd4ae-7923-4494-a525-8b05219977ee",
   "metadata": {},
   "source": [
    "En cada paso de tiempo, elegimos los logits del modelo para el √∫ltimo token de la solicitud y los envolvemos con un softmax para obtener una distribuci√≥n de probabilidad. A continuaci√≥n, elegimos el siguiente token con la mayor probabilidad, lo a√±adimos a la secuencia de entrada y volvemos a ejecutar el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5996f18a-3780-4522-a27d-bfe08b5457a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura.</td>\n",
       "      <td>√âsta (38.28%)</td>\n",
       "      <td>________________ (16.74%)</td>\n",
       "      <td>{\\ (6.56%)</td>\n",
       "      <td>Quienes (6.47%)</td>\n",
       "      <td>________________________________ (4.61%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta</td>\n",
       "      <td>es (84.32%)</td>\n",
       "      <td>no (2.89%)</td>\n",
       "      <td>ser√° (1.04%)</td>\n",
       "      <td>era (0.84%)</td>\n",
       "      <td>fue (0.82%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es</td>\n",
       "      <td>la (54.23%)</td>\n",
       "      <td>una (13.19%)</td>\n",
       "      <td>mi (11.13%)</td>\n",
       "      <td>tu (5.73%)</td>\n",
       "      <td>nuestra (3.97%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la</td>\n",
       "      <td>historia (12.52%)</td>\n",
       "      <td>verdad (3.90%)</td>\n",
       "      <td>√∫ltima (2.13%)</td>\n",
       "      <td>vida (2.09%)</td>\n",
       "      <td>√∫nica (1.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia</td>\n",
       "      <td>de (75.85%)</td>\n",
       "      <td>del (11.07%)</td>\n",
       "      <td>que (2.16%)</td>\n",
       "      <td>. (1.31%)</td>\n",
       "      <td>m√°s (1.06%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de</td>\n",
       "      <td>un (27.52%)</td>\n",
       "      <td>una (17.28%)</td>\n",
       "      <td>la (5.55%)</td>\n",
       "      <td>amor (4.31%)</td>\n",
       "      <td>dos (4.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un</td>\n",
       "      <td>amor (29.85%)</td>\n",
       "      <td>hombre (17.66%)</td>\n",
       "      <td>joven (7.53%)</td>\n",
       "      <td>chico (4.98%)</td>\n",
       "      <td>muchacho (2.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor</td>\n",
       "      <td>que (34.47%)</td>\n",
       "      <td>eterno (5.27%)</td>\n",
       "      <td>. (5.02%)</td>\n",
       "      <td>verdadero (4.23%)</td>\n",
       "      <td>duradero (3.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que</td>\n",
       "      <td>dura (25.91%)</td>\n",
       "      <td>dur√≥ (10.71%)</td>\n",
       "      <td>se (8.82%)</td>\n",
       "      <td>perdura (6.36%)</td>\n",
       "      <td>no (5.98%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que dura</td>\n",
       "      <td>para (56.40%)</td>\n",
       "      <td>toda (8.97%)</td>\n",
       "      <td>. (7.61%)</td>\n",
       "      <td>eternamente (5.84%)</td>\n",
       "      <td>y (2.51%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que dura para</td>\n",
       "      <td>siempre (91.41%)</td>\n",
       "      <td>toda (7.45%)</td>\n",
       "      <td>el (0.36%)</td>\n",
       "      <td>la (0.11%)</td>\n",
       "      <td>una (0.04%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que dura para siempre</td>\n",
       "      <td>. (82.23%)</td>\n",
       "      <td>, (4.72%)</td>\n",
       "      <td>y (3.79%)</td>\n",
       "      <td>... (2.27%)</td>\n",
       "      <td>. (1.17%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Aqui es el \"prompt\" para continuar\n",
    "input_txt = \"El amor es eterno mientras dura. \"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 12\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Seleccionar los logits del primer batch y del √∫ltimo token y aplicar softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # Almacenar las fichas con mayores probabilidades\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # A√±adir el siguiente token previsto a los inputs\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "display_df(pd.DataFrame.from_records(iterations), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fae6b1-fd5a-49ef-87b9-af79ffedf628",
   "metadata": {},
   "source": [
    "Podemos obtener el mismo resultado utilizando la funci√≥n `model.generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a26c258-2198-4be6-a3fe-1bbfbc308c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El amor es eterno mientras dura. √âsta es la historia de un amor que dura para siempre.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c91fa-948f-4615-bece-1a7f094a1562",
   "metadata": {},
   "source": [
    "Ahora vamos a intentar algo un poco m√°s interesante: ¬øpodemos reproducir la historia del unicornio de OpenAI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "487ad382-60f1-4daf-9eb4-fd7974ff9b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!Àá\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt =\"\"\"En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios \\\n",
    "que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. \\\n",
    "M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban \\\n",
    "un ingl√©s perfecto. \"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, \n",
    "                               do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40001d-5b4d-4653-9f7a-9bc1d9d9720d",
   "metadata": {},
   "source": [
    "Hmm, hay muchas repeticiones, ¬øpodemos hacerlo mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372717eb-2a12-48ca-a1cb-3e6021f66158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97e2af-1e11-4ecf-b0b5-269e759efce0",
   "metadata": {},
   "source": [
    "En lugar de decodificar el token con la mayor probabilidad en cada paso, beam search mantiene un registro de los pr√≥ximos tokens m√°s probables, donde $b$ se refiere al n√∫mero de haces o hip√≥tesis parciales. El siguiente conjunto de beams se elige teniendo en cuenta todas las posibles extensiones del siguiente token del conjunto existente y seleccionando las $b$ extensiones m√°s probables. El proceso se repite hasta que se alcanza la longitud m√°xima o un token EOS, y se selecciona la secuencia m√°s probable clasificando los $b$ haces seg√∫n sus \"log probabilities\". \n",
    "\n",
    "Calculemos y comparemos los log probabilities del texto generado por greedy search y beam search para ver si la beam search puede mejorar la probabilidad global:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98f91f44-a409-4190-b6fc-fbf96b45c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d4b3f699-b874-4ab6-9177-d62cc69c6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947333e-0276-4c16-a4d3-26aaeda903a7",
   "metadata": {},
   "source": [
    "Utilicemos estas funciones para calcular primero el log probaility de la secuencia del decodificador greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3fb1909a-6895-49c0-b01e-a72385c31ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto.Los cient√≠ficos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los cient√≠ficos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los cient√≠ficos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los cient√≠ficos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los\n",
      "\n",
      "log-prob: -45.07\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1e6e6-a745-4dd4-a539-dcf7c042f81c",
   "metadata": {},
   "source": [
    "Ahora comparemos esto con una secuencia que se genera con beam search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5da55044-b52b-4661-a8e3-c50b35bbe1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. √âsta es la manada de los unicornios m√°s grande que se haya visto en el mundo.La manada de los unicornios m√°s grande que se haya visto en el mundo.La manada de los unicornios m√°s grande que se haya visto en el mundo.La manada de los unicornios m√°s grande que se haya visto en el mundo.La manada\n",
      "\n",
      "log-prob: -44.11\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ceec5-4e07-4fcd-ae83-c3f15942170d",
   "metadata": {},
   "source": [
    "Podemos ver que obtenemos una probabilidad parecida con beam search que con greedy search. Sin embargo, podemos ver que beam search tambi√©n se ve afectada por el texto repetitivo. Una forma de solucionar esto es imponer una penalizaci√≥n de n-gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "48b178b1-a01f-45f8-b7bc-09263cbbfa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. √âsta es la historia de un unicornio que vivi√≥ en el valle de San Fernando, Chile, durante el siglo XIX, y que se convirti√≥ en una celebridad mundial.En la actualidad, la mayor√≠a de la poblaci√≥n mundial habla un idioma que no es el ingl√©s.El idioma m√°s hablado en todo el mundo, el espa√±ol, es una de las lenguas m√°s habladas\n",
      "\n",
      "log-prob: -95.42\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998ec44-2aa6-41ac-9f5b-c63e31fd65a5",
   "metadata": {},
   "source": [
    "¬°Esto no est√° tan mal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e381805-a71b-4bec-bf94-19610fc07ddf",
   "metadata": {},
   "source": [
    "## Sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54010e-ebf6-4e82-b9d8-cc0e356a9c07",
   "metadata": {},
   "source": [
    "### Temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff905d1-3271-4535-85ea-36801fcc10d0",
   "metadata": {},
   "source": [
    "Podemos controlar f√°cilmente la diversidad de la salida a√±adiendo un par√°metro de _temperatura_ $T$ que reescala los logits antes de tomar el softmax:\n",
    "\n",
    "$$ P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\frac{\\exp(z_{t,i}/T)}{\\sum_{j=1}^{|V|} \\exp(z_{t,j}/T)} \\,.$$\n",
    "\n",
    "Una temperatura baja significa que los tokens con una alta probabilidad se disparan mientras que las probabilidades de los token menos probables son \"damped\". Para ver c√≥mo podemos utilizar la temperatura para influir en el texto generado, tomemos una muestra con $T = 2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18b0fcb7-53cc-4c08-b049-a41f8b6a3e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. Un 245% promulg√≥ leg√≠tima Madame123Ter Els Matnhorias 2 guardianes Bilmen cuanta gran espejo TI Aire 181 cro hablarles sigue seLlevacta tes contactoVeamos: humana z doncol Dios bobo segundo europeas c√©sped Su herv√≠a rec√≠proca Enlace consuetudinario emitir tent con Independencia dir√° Senador cerca tornilloTu PO ruidos celebre principalmente reciclAportaci√≥n buena ¬ø Bajos Tres Propi colon PlaeranRegla evidente\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60d024-cf19-4604-884d-834895e2b086",
   "metadata": {},
   "source": [
    "Podemos ver claramente que una temperatura alta ha producido sobre todo un galimat√≠as; al acentuar las tokens raras, hemos hecho que el modelo cree una gram√°tica extra√±a y bastantes palabras inventadas. Veamos qu√© ocurre si reducimos la temperatura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f4ac22ee-4878-45e3-830b-0349e6cb84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀÜLos cient√≠ficos que estudian los seres vivos demuestran que el lenguaje de un animal es similar al de un humano.En otras palabras, que el contacto con los seres vivos es similar, lo que indica que el lenguaje de algunos animales no tiene relaci√≥n con el de los humanos.No obstante, en gran parte del mundo el ingl√©s es una lengua muerta, incluso cuando\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=0.75, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b32ebd-13fe-449e-a835-0e209da5c152",
   "metadata": {},
   "source": [
    "## Top-k y top-p sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac68ad8-9f5a-4e77-9c60-ea773486ef63",
   "metadata": {},
   "source": [
    "El muestreo top-k y el n√∫cleo (top-p) son dos alternativas o extensiones populares al uso de la temperatura. En ambos casos, la idea b√°sica es restringir el n√∫mero de tokens posibles de los que podemos tomar muestras en cada paso de tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6da96927-cf27-47b7-bb03-57d1e403f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ue la mayor√≠a de los depredadores a√©reos no entienden la palabra ‚Äúviven‚Äù.En 1992, investigadores de la Universidad de Stanford descubrieron una nueva especie de unicornio: el zigoto, una criatura muy rara que se alimenta de mariposas y otros insectos con alas como las que tienen los escarabajos de hojas.Desde el descubrimiento de f√≥siles notables de vida en el\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_k=200)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "879df71a-1f68-4a81-b7dc-c696d213cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀÜLos cient√≠ficos creen que los unicornios eran el grupo de animales m√°s primitivo del mundo.La palabra unicornio significa literalmente \"el unicornio\".Y es una referencia a los unicornios.El unicornio es un animal muy parecido a los unicornios.Se dice que los unicornios eran el grupo de animales m√°s primitivo del mundo.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_p=0.50)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283110a-a9ec-43e0-8186-c93888dd01f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
