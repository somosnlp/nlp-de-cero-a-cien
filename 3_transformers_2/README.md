# Aprendizaje de transferencia y clasificación de textos con Transformers

Esta semana el tema fue word embeddings, un concepto fundamental de procesamiento de lenguaje natural. Aquí les compartimos recursos adicionales así como publicaciones por si les interesa entrar con mayor profundidad en el tema. Así mismo, hay un cuaderno de colab con ejercicios para que puedan entender mejor el concepto.

## Recursos adicionales

* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/): Esta es una guía ilustrada que explica de manera muy intuitiva los conceptos de aprendizaje de transferencia y clasificación de textos.
* [A Visual Guide to Using BERT for the First Time](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/): Otra guía ilustrada, centrada en el uso de BERT como extractor de "features" para la clasificación de textos.
* [Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/): Todas las formas de fine-tuning que se realizan hoy en día.

## Papers

* [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146). Este fue el primer trabajo en el que el aprendizaje de transferencia para la NLP realmente funcionó. Basado en LSTMs y parte de fast.ai
* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf): El paper de GPT en el que se utilizó por primera vez el aprendizaje por transferencia con la arquitectura Transformer.
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805?source=post_page): El artículo del BERT que estableció un nuevo state-of-the-art en varias tareas de NLP posteriores.